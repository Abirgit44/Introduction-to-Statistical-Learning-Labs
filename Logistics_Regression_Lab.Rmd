1.  <div>

    1.  <div>

        ---
        title: 'Lab: Logistic Regression, LDA, QDA, and KNN'
        output:
          pdf_document: default
          html_notebook: default
        ---

        </div>

    </div>

# Stock Market Data 

```{r}
#importing the library
library(ISLR)
names(Smarket)
```

```{r}
#Stock Market Dataset shape
dim(Smarket)
```

```{r}
summary(Smarket)
```

```{r}
#pairplot
pairs(Smarket)
```

```{r}
# The command below gives an error message because the Direction variable is qualitative.

#cor(Smarket)
```

```{r}
#updating it
cor(Smarket[,-9])
```

```{r}
#plotting the dataset
attach(Smarket)
plot(Volume)
```

# Logistics Regression 

```{r}
#Next, we will ft a logistic regression model in order to predict Direction using Lag1 through Lag5 and Volume. 
glm.fits = glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume, data=Smarket,family=binomial)
summary(glm.fits)
```

```{r}
#coefficients
coef(glm.fits)
```

```{r}
summary(glm.fits)$coef
```

```{r}
summary(glm.fits)$coef[,4]
```

```{r}
#The predict() function can be used to predict the probability that the market will go up, given values of the predictors. The type = "response" option tells R to output probabilities of the form P(Y = 1|X)
glm.probs=predict(glm.fits,type="response")
glm.probs[1:10]
```

```{r}
#contrasts() function indicates that R has created a dummy variable with a 1 for Up.
contrasts(Direction)
```

```{r}
#The following two commands create a vector of class predictions based on whether the predicted probability of a market increase is greater than or less than 0.5
glm.pred=rep("Down",1250)
glm.pred[glm.probs>.5]="Up"
```

```{r}
table(glm.pred,Direction)
```

```{r}
#prediction rate
(507+145)/1250
```

```{r}
#mean
mean(glm.pred==Direction)
```

```{r}
#training for less that Year 2005
train=(Year<2005)
Smarket.2005=Smarket[!train,]
dim(Smarket.2005)
```

```{r}
Direction.2005=Direction[!train]
glm.fits=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,data=Smarket,family=binomial,subset=train)
```

```{r}
glm.probs=predict(glm.fits,Smarket.2005,type="response")
glm.pred=rep("Down",252)
glm.pred[glm.probs>.5]="Up"
```

```{r}
table(glm.pred,Direction.2005)
```

```{r}
mean(glm.pred==Direction.2005)
```

```{r}
mean(glm.pred!=Direction.2005)
```

```{r}
#Below we have reft the logistic regression using just Lag1 and Lag2, which seemed to have the highest predictive power in the original logistic regression model.
glm.fits=glm(Direction~Lag1+Lag2,data=Smarket,family=binomial,subset=train)
glm.probs=predict(glm.fits,Smarket.2005,type="response")
glm.pred=rep("Down",252)
glm.pred[glm.probs>.5]="Up"
```

```{r}
table(glm.pred,Direction.2005)
```

```{r}
mean(glm.pred==Direction.2005)
```

```{r}
106/(106+76)
```

```{r}
predict(glm.fits,newdata=data.frame(Lag1=c(1.2,1.5),Lag2=c(1.1,-0.8)),type="response")
```

# Linear Discriminant Analysis 

### 

Now we will perform LDA on the Stock market data

```{r}
library(MASS)
#fitting the model
lda.fit=lda(Direction~Lag1+Lag2,data=Smarket,subset=train)
lda.fit
```

```{r}
plot(lda.fit)
```

```{r}
lda.pred=predict(lda.fit, Smarket.2005)
names(lda.pred)
```

```{r}
lda.class=lda.pred$class
table(lda.class,Direction.2005)
```

```{r}
mean(lda.class==Direction.2005)
sum(lda.pred$posterior[,1]>=.5)
```

```{r}
#Notice that the posterior probability output by the model corresponds to the probability that the market will decrease:

sum(lda.pred$posterior[,1]<.5)
lda.pred$posterior[1:20,1]
```

```{r}
lda.class[1:20]
```

```{r}
sum(lda.pred$posterior[,1]>.9)
```

# Quadratic Discriminant Analysis 

```{r}
qda.fit=qda(Direction~Lag1+Lag2,data=Smarket,subset=train)
qda.fit
```

```{r}
qda.class=predict(qda.fit,Smarket.2005)$class
table(qda.class,Direction.2005)
```

```{r}
mean(qda.class==Direction.2005)
```

# K-Nearest Neighbors 

Here we have gone through the procedure of creating KNN classifier and seeing the outputs from it.

```{r}
library(class)
```

```{r}
train.X=cbind(Lag1,Lag2)[train,]
test.X=cbind(Lag1,Lag2)[!train,]
train.Direction=Direction[train]
set.seed(1)
```

```{r}
knn.pred=knn(train.X,test.X,train.Direction,k=1)
table(knn.pred,Direction.2005)
```

```{r}
(83+43)/252
```

```{r}
knn.pred=knn(train.X,test.X,train.Direction,k=3)
table(knn.pred,Direction.2005)
```

```{r}
mean(knn.pred==Direction.2005)
```

# An Application to Caravan Insurance Data 

Here we have gone through the process of seeing an application to Caravan Insurance Data from what we learnt.

```{r}
dim(Caravan)
```

```{r}
attach(Caravan)
summary(Purchase)
```

```{r}
348/5822
```

```{r}
standardized.X=scale(Caravan[,-86])
var(Caravan[,1])
```

```{r}
var(Caravan[,2])
```

```{r}
var(standardized.X[,1])
var(standardized.X[,2])
```

```{r}
test=1:1000
train.X=standardized.X[-test,]
test.X=standardized.X[test,]
train.Y=Purchase[-test]
test.Y=Purchase[test]
set.seed(1)
knn.pred=knn(train.X,test.X,train.Y,k=1)
```

```{r}
mean(test.Y!=knn.pred)
```

```{r}
mean(test.Y!="No")
```

```{r}
table(knn.pred,test.Y)
```

```{r}
9/(68+9)
```

```{r}
knn.pred=knn(train.X,test.X,train.Y,k=3)
table(knn.pred,test.Y)
```

```{r}
5/26
```

```{r}
knn.pred=knn(train.X,test.X,train.Y,k=5)
table(knn.pred,test.Y)
```

```{r}
4/15
```

```{r}
glm.fits=glm(Purchase~.,data=Caravan,family=binomial,subset=-test)
glm.probs=predict(glm.fits,Caravan[test,],type="response")
glm.pred=rep("No",1000)
glm.pred[glm.probs>.5]="Yes"
```

```{r}
table(glm.pred,test.Y)
```

```{r}
glm.pred=rep("No",1000)
glm.pred[glm.probs>.25]="Yes"
table(glm.pred,test.Y)
```

```{r}
11/(22+11)
```

So from both KNN and Logistic Regression we can see the Logistic Regression gives better output.

#### End of Classification Lab
